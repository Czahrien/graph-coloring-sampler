\documentclass[14]{article}
\usepackage[margin=.75in]{geometry}
 \usepackage{setspace}
\usepackage{amsmath}
\usepackage{enumerate}
\setlength{\parindent}{0in}
\begin{document}
\title{Project Summary}
\author{Tony Bentancur \texttt{<amb8241@rit.edu>}\\ Ben Mayes \texttt{<bdm8233@rit.edu>}\\ Josh Lindsay \texttt{<jal3040@rit.edu>}}
\date{\today}
\maketitle
\section{Reduction of Counting to Sampling}
Recall form homework 5 (with some minor modifications):	
\begin{itemize}
\item $\displaystyle G_i = $ the graph $G$ with only edges incident to vertices in $\{v_0, v_1, \ldots ,v_{i-1}\} \subseteq V$
\item $\displaystyle\rho_i = \frac{\mbox{number of colorings in } G_{i+1} }{\mbox{number of colorings in } G_{i} } = \frac{|G_{i+1}|}{|G_{i}|}$
\item $\displaystyle\frac{q-\Delta}{q} \leq \rho_i \leq 1$
\item $\displaystyle\prod_{i=1}^{n-1}{\rho_i} = \frac{|C(G_n)|}{q^n}$
\end{itemize}

To determine the number of samples the reduction was identical to the reduction of edge matching counting operating on the assumption that $q \geq 2*\Delta$. When $q$ is less than this it is difficult, if not impossible to even make conclusions about the mixing time of the Markov chain, this means that merely knowing if the Markov chain has mixed and is indeed sampling randomly with respect to the initial coloring which defeats the entire purpose of sampling using an FPAUS because we do not even know if it is sampling uniformly. Using the assumption that $q \geq 2*\Delta$ we can make the following simplifying assumption:

\[ \frac{q-\Delta}{q} \geq \frac{\Delta}{2\Delta} \geq \frac{1}{2} \]

This assumption allows us to bound $\rho_i$ as follows:

\[ \frac{1}{2} \leq \rho_i \leq 1 \]

Which means that nearly all the calculations of the edge matching sampling chain can be used except for the number of samples (which relies on the number of $\rho_i$ terms). However, all that needs to change there is the $m$ terms in the matchings to the $n$ terms in the colorings. Since none of the future steps rely on $m$ in any way that substituting it with $n$ will break it, this works out quite nicely. This means that the number of samples, $s$, becomes:

\[ s = \frac{75*n}{\epsilon^2} \]

Aside from that change, the reductions are both identical.

\section{Instructions}
To build the two main programs, \texttt{count\_colorings} (the brute-force coloring counter) and \texttt{sampler} (the counting sampler) simply type \texttt{make} in the main directory. To build \texttt{gen\_graph}, the graph generator, go to the input directory and type \texttt{make}. These programs are run as followed:
\begin{itemize}
\item \texttt{count\_colorings}: \texttt{./count\_colorings input} - Where input is the input file.
\item \texttt{sampler}: \texttt{./sampler input [epsilon]} - Where input is the input file and epsilon is an optional parameter in the interval $(0,1]$ which defaults to $0.1$ if not given.
\item \texttt{gen\_graph}: \texttt{./gen\_graph n q [p]} - Where n is the number of vertices, q is the number of colors, and p is an optional probability of including each edge in $K_n$.
\end{itemize}
\section{Input}
\subsection{Other Students}
Since our sampler is rather optimized it handles graphs with less than 50 vertices quite effectively (each run on a k50 with 100 colors takes approximately one minute to complete). However, Due to the quartic nature of $O(\frac{n^4}{\epsilon^2})$ doubling this number to 100 however means that the running time is multiplied by a factor of 16 (one of James' 100 vertex graphs completed a trial in approximately 16 minutes which agrees with this intuition). Couple this with the fact of the counting being done 7 times to raise the probability of being within $\pm \frac{\epsilon}{2}$ of the actual answer to $0.9$ and you have something that runs for over an hour. Due to time constraints these graphs of $n \geq 100$ could not be completed.
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Input File & Approximate Number of Colorings & Running Time \\\hline
\texttt{Bogue, Max - test1.txt} & 5.36113e+25 & 56.191s\\\hline
\texttt{Bogue, Max - test2.txt} & 1.31716e+37 & 202.974s\\\hline
\texttt{Bogue, Max - test3.txt} & 2.25976e+36 & 443.534s\\\hline
\texttt{Lange, Alexander - high.txt} & 1.60548e+12 & 6.871s\\\hline
\texttt{Lange, Alexander - mid.txt} & 1.90918e+12 & 4.850s\\\hline
\texttt{Lange, Alexander - low.txt} & 1.69701e+11 & 6.119s\\\hline
\texttt{Leibig, Brian - dense.graph} & 6.34569e+22 & 12.954s\\\hline
\texttt{Leibig, Brian - normal.graph} & 6.87353e+19 & 9.024s\\\hline
\texttt{Leibig, Brian - sparse.graph} & 7.64573e+16 & 4.650s\\\hline
\texttt{Loomis, James - graph-25-47-8} & 6.02073e+19 & 27.290s\\\hline
\texttt{Loomis, James - graph-50-1000-45} & 8.87037e+69 & 578.826s\\\hline
\texttt{Mayes, Benjamin - dense.in} & 2.4712e+11 & 1.510s \\\hline
\texttt{Mayes, Benjamin - k5.in} & 95683 & 0.234s\\\hline
\texttt{Mayes, Benjamin - sparse.in} & 130312 & 0.138s\\\hline
\texttt{Okka - highQ highM.txt} & 2.15877e+06 & 1.230s\\\hline
\texttt{Okka - mediumQ sparseM.txt} & 104420 & 0.566s\\\hline
\texttt{Okka - lowQ mediumM.txt} & 2.05731e+07 & 2.005s\\\hline
\texttt{Stalnaker, David - dense.txt}  &  4.66076e+71 & 502.099s\\\hline
\texttt{Stalnaker, David - sparse.txt} & 1.37444e+50 & 181.967s \\\hline
\texttt{Stalnaker, David - medium.txt} &  1.50574e+71 &419.767s\\\hline
\end{tabular}
\end{center}
\subsection{Our Inputs}
\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l | }
    \hline
    \textbf{Filename} & \textbf{n} & \textbf{m} & \textbf{q} & \textbf{Characteristics} & \textbf{Brute Force} & \textbf{Estimation} \\ \hline
    n5q18p0.25.in & 5 & 2 & 18 &sparse & 1685448 in 2.289s &1.68667e+06 in 0.098s   \\ \hline

    n8q8p0.75.in & 8 & 23 & 8 &dense & 349440 in 18.277s &355841 in 1.184s \\ \hline

    n7q15p0.85.in & 7 & 19 & 15 &dense & 39999960 in 3m27.6s & 4.00214e+07 in 0.847s\\ \hline

    n5q7p0.88.in & 5 & 9 & 7 &dense & 3360 in .021s & 3375.63 in 0.209s    \\ \hline

    n8q5p0.15.in & 8 & 7 & 5 &sparse & 72000 in .391s & 72231.6  in 0.583s  \\ \hline

    n7q7p1.00.in & 7 & 21 & 7 &complete & 5040 in .804s &4910.67  in 0.904s\\ \hline

    n9q9p1.00.in & 9 & 36 & 9 &complete & 362880 in 5m8.299s &360473 in 2.214s   \\ \hline

    reg8v7e5c.in & 8 & 7 & 5 & 2-regular & 81920 in 0.088s & 82270.3 in 0.616s \\ \hline

    reg10v15e7c.in & 10 & 15 & 7 & 3-regular & 27767880 in 1m12.817s & 2.72594e+07 in 1.160s \\ \hline

    reg6v9e11c.in & 6 & 9 & 11 & 3-regular & 738540 in 0.356s & 736833 in 0.284\\ \hline
    \end{tabular}
\end{center}
Most of our inputs were generated using a graph generation program (see \texttt{gen\_graph} in the submission's input directory). This program takes in a number of vertices, a number of colors, and an optional probability (it defaults to 1.0) of randomly including any given edge. This results in a graph that is dense if the given probability is close to 1, a sparse graph if the probability is close to 0, and a medium graph if the probability is somewhere in-between. The three regular graph inputs were generated by hand and the 2-regular graph is a simple cycle while the 3-regular graph with 10 vertices happens to be the Peterson graph. After several trials I arrived at the following graphs:
\begin{itemize}
\item \texttt{sparse.in} - a 5 vertex graph with $p=0.4$.
\item \texttt{k5.in} - a 5 vertex complete graph.
\item \texttt{dense.in} - a 10 vertex graph with $p=0.6$.
\end{itemize}
Note that at the time of generation I was expecting these to be feasibly run by the brute force algorithm (although it may require longer than most people care to run it for the dense graph) so they are quite a bit smaller than they could be. The benefit of doing this however is that it is easy to verify that the approximated number of colorings are close to the correct number.
\section{Experimentation}
The brute force approach we took (use executable file \texttt{count\_colorings}) that we took is not very optimized and we did manage to run into several input files that overflowed either the product or the count.
Varying epsilon can have dramatic results due to the inverse square law (we sample proportionally to the inverse of $\epsilon^2$. For instance, halving epsilon means a quadrupling in run-time and double epsilon means a forthing of run-time. This means that very high epsilons can be used for very quick estimations and much smaller epsilons can be used when these estimations do not suffice. Despite the theoretical results stating that an epsilon of 0.9 is inaccurate, on a complete graph it tends to estimate very accurately (it is usually within $10-20\%$ of the actual value) but outliers tend to be extreme. Whether or not this holds for all graphs is hard to say, given more time we would determine more.
\section{Conclusions}
\subsection{Accuracy}
The approximations done by this counting approach can be very accurate. When testing initially we used several complete graphs with double the number of colors of vertices. This means that the colorings were equal to $\frac{(2n)!}{n!}$ by a basic combinatorics argument. This sampler tends to produce an answer that is more accurate than the theoretical bounds, however we cannot use this to our advantage without modifying some of the weaker bounds taken in the sampling reduction and testing our sampler on a wide variety of graphs of sizes and structures whereas most of our testing has been done on complete graphs and the other students' input graphs.
\subsection{Ease of Writing and Optimization}
Despite some of the lengthy statistical mathematical reasoning behind this sort of of approach, the actual implementation of the counting algorithm is very simple and very optimizable. We were able to get decent running times by using an adjacency matrix and using only an $i$-by-$i$ sub-matrix to greatly decrease times as smaller sub-graphs are sampled from. Since this means that the underlying graph data structure changes in no manner other than its colors (which can be made into a temporary variable) this approach lends itself to parallelization provided that the cost of communication between processors/machines does not exceed the benefit gained from parallelization.

\end{document}