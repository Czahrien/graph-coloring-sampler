\documentclass[14]{article}
\usepackage[margin=.75in]{geometry}
 \usepackage{setspace}
\usepackage{amsmath}
\usepackage{enumerate}
\setlength{\parindent}{0in}
\begin{document}
\title{Project Summary}
\author{Tony Bentancur \texttt{<amb8241@rit.edu>}\\ Ben Mayes \texttt{<bdm8233@rit.edu>}\\ Josh Lindsay \texttt{<jal3040@rit.edu>}}
\date{\today}
\maketitle
\section{Reduction of Counting to Sampling}
Recall form homework 5 (with some minor modifications):	
\begin{itemize}
\item $\displaystyle G_i = $ the graph $G$ with only edges incident to vertices in $\{v_0, v_1, \ldots ,v_{i-1}\} \subseteq V$
\item $\displaystyle\rho_i = \frac{\mbox{number of colorings in } G_{i+1} }{\mbox{number of colorings in } G_{i} } = \frac{|G_{i+1}|}{|G_{i}|}$
\item $\displaystyle\frac{q-\Delta}{q} \leq \rho_i \leq 1$
\item $\displaystyle\prod_{i=1}^{n-1}{\rho_i} = \frac{|C(G_n)|}{q^n}$
\end{itemize}

To determine the number of samples the reduction was identical to the reduction of edge matching counting operating on the assumption that $q \geq 2*\Delta$. When $q$ is less than this it is difficult, if not impossible to even make conclusions about the mixing time of the Markov chain, this means that merely knowing if the Markov chain has mixed and is indeed sampling randomly with respect to the initial coloring which defeats the entire purpose of sampling using an FPAUS because we do not even know if it is sampling uniformly. Using the assumption that $q \geq 2*\Delta$ we can make the following simplifying assumption:

\[ \frac{q-\Delta}{q} \geq \frac{\Delta}{2\Delta} \geq \frac{1}{2} \]

This assumption allows us to bound $\rho_i$ as follows:

\[ \frac{1}{2} \leq \rho_i \leq 1 \]

Which means that nearly all the calculations of the edge matching sampling chain can be used except for the number of samples (which relies on the number of $\rho_i$ terms). However, all that needs to change there is the $m$ terms in the matchings to the $n$ terms in the colorings. Since none of the future steps rely on $m$ in any way that substituting it with $n$ will break it, this works out quite nicely. This means that the number of samples, $s$, becomes:

\[ s = \frac{75*n}{\epsilon^2} \]

Aside from that change, the reductions are both identical.

\section{Input}
\subsection{Other Students}
Since our sampler is rather optimized it handles graphs with less than 50 vertices quite effectively (each run on a k50 with 100 colors takes approximately one minute to complete). However, Due to the quartic nature of $O(\frac{n^4}{\epsilon^2})$ doubling this number to 100 however means that the running time is multiplied by a factor of 16 (one of James' 100 vertex graphs completed a trial in approximately 16 minutes which agrees with this intuition). Couple this with the fact of the counting being done 7 times to raise the probability of being within $\pm \frac{\epsilon}{2}$ of the actual answer to $0.9$ and you have something that runs for over an hour. Due to time constraints these graphs of $n \geq 100$ could not be completed.
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Input File & Approximate Number of Colorings & Running Time \\\hline
\texttt{Bogue, Max - test1.txt} & 5.36113e+25 & 56.191s\\\hline
\texttt{Bogue, Max - test2.txt} & 1.31716e+37 & 202.974s\\\hline
\texttt{Bogue, Max - test3.txt} & 2.25976e+36 & 443.534s\\\hline
\texttt{Lange, Alexander - high.txt} & 1.60548e+12 & 6.871s\\\hline
\texttt{Lange, Alexander - mid.txt} & 1.90918e+12 & 4.850s\\\hline
\texttt{Lange, Alexander - low.txt} & 1.69701e+11 & 6.119s\\\hline
\texttt{Leibig, Brian - dense.graph} & 6.34569e+22 & 12.954s\\\hline
\texttt{Leibig, Brian - normal.graph} & 6.87353e+19 & 9.024s\\\hline
\texttt{Leibig, Brian - sparse.graph} & 7.64573e+16 & 4.650s\\\hline
\texttt{Loomis, James - graph-25-47-8} & 6.02073e+19 & 27.290s\\\hline
\texttt{Loomis, James - graph-50-1000-45} & 8.87037e+69 & 578.826s\\\hline
\texttt{Mayes, Benjamin - dense.in} & 2.4712e+11 & 1.510s \\\hline
\texttt{Mayes, Benjamin - k5.in} & 95683 & 0.234s\\\hline
\texttt{Mayes, Benjamin - sparse.in} & 130312 & 0.138s\\\hline
\texttt{Okka- highQ highM.txt} & 2.15877e+06 & 1.230s\\\hline
\texttt{Okka- mediumQ sparseM.txt} & 104420 & 0.566s\\\hline
\texttt{Okka- lowQ mediumM.txt} & 2.05731e+07 & 2.005s\\\hline
\texttt{Stalnaker, David - dense.txt}  &  4.66076e+71 & 502.099s\\\hline
\texttt{Stalnaker, David - sparse.txt} & 1.37444e+50 & 181.967s \\\hline
\texttt{Stalnaker, David - medium.txt} &  1.50574e+71 &419.767s\\\hline
\end{tabular}
\end{center}
\subsection{Our Inputs}
All of three of our inputs were generated using a graph generation program (see \texttt{gen\_graph} in the submission's input directory). This program Takes in a number of vertices, a number of colors, and an optional probability (it defaults to 1.0) of randomly including any given edge. This results in a graph that is dense if the given probability is close to 1, a sparse graph if the probability is close to 0, and a medium graph if the probability is somewhere in-between. After several trials I arrived at the following graphs:
\begin{itemize}
\item \texttt{sparse.in} - a 5 vertex graph with $p=0.4$.
\item \texttt{k5.in} - a 5 vertex complete graph.
\item \texttt{dense.in} - a 10 vertex graph with $p=0.6$.
\end{itemize}
Note that at the time of generation I was expecting these to be feasibly run by the brute force algorithm (although it may require longer than most people care to run it for the dense graph) so they are quite a bit smaller than they could be. The benefit of doing this however is that it is easy to verify that the approximated number of colorings are close to the correct number.
\section{Experimentation}
Varying epsilon can have dramatic results due to the inverse square law (we sample proportionally to the inverse of $\epsilon^2$. For instance, halving epsilon means a quadrupling in run-time and double epsilon means a fourthing of run-time. This means that very high epsilons can be used for very quick estimations and much smaller epsilons can be used when these estimations do not suffice. Despite the theoretical results stating that an epsilon of 0.9 is inaccurate, on a complete graph it tends to estimate very accurately (it is usually within $10-20\%$ of the actual value) but outliers tend to be extreme. Whether or not this holds for all graphs is hard to say, given more time we would determine more.
\section{Conclusions}
\subsection{Accuracy}
The approximations done by this counting approach can be very accurate. When testing initially we used several complete graphs with double the number of colors of vertices. This means that the colorings were equal to $\frac{(2n)!}{n!}$ by a basic combinatorics argument. This sampler tends to produce an answer that is more accurate than the theoretical bounds, however we cannot use this to our advantage without modifying some of the weaker bounds taken in the sampling reduction and testing our sampler on a wide variety of graphs of sizes and structures whereas most of our testing has been done on complete graphs and the other students' input graphs.
\end{document}